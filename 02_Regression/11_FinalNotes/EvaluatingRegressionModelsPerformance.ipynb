{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember, that using the Adjusted R Squared is the best. It penalizes when you are adding more variables. Whe oyu add more variables, you're R-Squared will always increase because of R_Squared = 1 - (SSres/SStot)\n",
    "- SS Res is the difference btw the actual and difference\n",
    "- SS Tol is the total difference btw the actual and average. Hence, the average is sort of the baseline or the expected deviation. Now, we make our measure of our model of how close the predicted is to the actual. \n",
    "- If we want our model to do better, the ratio of SSRes/SSTot should be as low as possible. To be as low possible, we would decrease the denominator.\n",
    "- Now, the adjusted R Squared penalized the formula for having more regressors. Thus, when we add more regressors, we would for the 1-(SSRes/SSTot) to be robust or else, we are hurting our model\n",
    "    - Formula: 1 - (1-R^Squared)(n-1/(n-p-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The thing about looking at the coefficeint is that we cannot be strucked by their absolute values\n",
    "- Instead, look at it per unit because the magntitude can be measured in many different terms, hence, we cannot compare them btw variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.superdatascience.com/wp-content/uploads/2017/02/Regression-Pros-Cons.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
