{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- csv: delimited by a comma\n",
    "- tsv: delimited by a tab\n",
    "- The bad thing about using a tab is that it'll consider all the commas and not the specific commas that delimites the dataset!\n",
    "- In this case,we would rarely find a tab in a review (which is the datset that we are looking at)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "# Delimitted by tabs, and ignore the double quotes (just in case)\n",
    "df = pd.read_csv(\"../../archive/Restaurant_Reviews.tsv\", delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick look on the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will creating a bag of words to bette understand words that are signficant\n",
    "# We will apply stemming (to have fewer group by grouping past, present, and future tense words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Downloaded all the words that are considered an article!\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Eliminating article words (the, that, this)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Grouping words that are part of the same family (loved, love, will love)\n",
    "# bc their 'stem' value is identical and we need to optimize the model (too much words can be inefficient)\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the dataframe (removing everything except letters) for one of the examples\n",
    "import re\n",
    "\n",
    "# Define: we do not want to remove characters that are in a-z & A-Z\n",
    "review = re.sub('[^a-zA-Z]', ' ', df.loc[0, 'Review']) # The second parameter indicates the replacement value\n",
    "\n",
    "# We will change the reviews to lowercase\n",
    "review = review.lower()\n",
    "\n",
    "# Separating the review into words\n",
    "review = review.split()\n",
    "\n",
    "# Comparing the stopwwords with the words in our dataset (This should be commented out \n",
    "# The next if statement is used instead!\n",
    "\n",
    "# review = [word for word in review if not word in set(stopwords.words('english'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grouping similar words\n",
    "ps = PorterStemmer()\n",
    "\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "# loved became love! Keeping the root of the word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Joining the new set of words\n",
    "review = ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We created the frameword (let create a for loop)\n",
    "corpus = [] # A collection of texts \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    review = row['Review']\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review) # The second parameter indicates the replacement value\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLP: Tokenazation: Using the words of each review and making it a unique column\n",
    "# We will predict if a review will be negative or positive (since we have the results!)\n",
    "# Independent Variables: Each of the word\n",
    "# This is classification! We have indepdent variables to train a binary outcome (which is this example)\n",
    "# Bc we will be using each word as indepdent variable, this is the reason we needed to clean up the data!\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Removing the less relevant words (keeping 1500)\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "Xs = cv.fit_transform(corpus).toarray() # (1000 rows, 1565 words)\n",
    "\n",
    "y = df.loc[:, 'Liked'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Must decide which model will help us train the model\n",
    "# We will use the Bayes Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the data set into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the classifier to the training set\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the values of the testing set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pos: 91\n",
      "false pos: 42\n",
      "true neg: 55\n",
      "false neg: 12\n",
      "\n",
      "\n",
      "Out of 200 reviews, the model got 146 correct,\n",
      "Accuacy is: 0.73%\n",
      "Precision is: 0.68%\n",
      "Recall is: 0.88%\n",
      "F1 Score is: 0.77%\n"
     ]
    }
   ],
   "source": [
    "# Checking for the results\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "total = tn+fp+fn+tp\n",
    "accuracy = (tn+tp)/total\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"true pos: {0}\\n\"\n",
    "    \"false pos: {1}\\n\"\n",
    "    \"true neg: {2}\\n\"\n",
    "    \"false neg: {3}\\n\".format(tp, fp, tn, fn))\n",
    "\n",
    "print(\"\"\"\n",
    "Out of {0} reviews, the model got {1} correct,\n",
    "Accuacy is: {2:.2f}%\n",
    "Precision is: {3:.2f}%\n",
    "Recall is: {4:.2f}%\n",
    "F1 Score is: {5:.2f}%\"\"\".format(total, tn+tp, accuracy, precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisions Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the classifier to the training set\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the values of the testing set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pos: 68\n",
      "false pos: 23\n",
      "true neg: 74\n",
      "false neg: 35\n",
      "\n",
      "\n",
      "Out of 200 reviews, the model got 142 correct,\n",
      "Accuacy is: 0.71%\n",
      "Precision is: 0.75%\n",
      "Recall is: 0.66%\n",
      "F1 Score is: 0.70%\n"
     ]
    }
   ],
   "source": [
    "# Checking for the results\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "total = tn+fp+fn+tp\n",
    "accuracy = (tn+tp)/total\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"true pos: {0}\\n\"\n",
    "    \"false pos: {1}\\n\"\n",
    "    \"true neg: {2}\\n\"\n",
    "    \"false neg: {3}\\n\".format(tp, fp, tn, fn))\n",
    "\n",
    "print(\"\"\"\n",
    "Out of {0} reviews, the model got {1} correct,\n",
    "Accuacy is: {2:.2f}%\n",
    "Precision is: {3:.2f}%\n",
    "Recall is: {4:.2f}%\n",
    "F1 Score is: {5:.2f}%\"\"\".format(total, tn+tp, accuracy, precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the classifier to the training set\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=4, random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the values of the testing set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pos: 40\n",
      "false pos: 4\n",
      "true neg: 93\n",
      "false neg: 63\n",
      "\n",
      "\n",
      "Out of 200 reviews, the model got 133 correct,\n",
      "Accuacy is: 0.67%\n",
      "Precision is: 0.91%\n",
      "Recall is: 0.39%\n",
      "F1 Score is: 0.54%\n"
     ]
    }
   ],
   "source": [
    "# Checking for the results\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "total = tn+fp+fn+tp\n",
    "accuracy = (tn+tp)/total\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"true pos: {0}\\n\"\n",
    "    \"false pos: {1}\\n\"\n",
    "    \"true neg: {2}\\n\"\n",
    "    \"false neg: {3}\\n\".format(tp, fp, tn, fn))\n",
    "\n",
    "print(\"\"\"\n",
    "Out of {0} reviews, the model got {1} correct,\n",
    "Accuacy is: {2:.2f}%\n",
    "Precision is: {3:.2f}%\n",
    "Recall is: {4:.2f}%\n",
    "F1 Score is: {5:.2f}%\"\"\".format(total, tn+tp, accuracy, precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMS (Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the classifier to the training set\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear', random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the values of the testing set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pos: 70\n",
      "false pos: 23\n",
      "true neg: 74\n",
      "false neg: 33\n",
      "\n",
      "\n",
      "    Out of 200 reviews, the model got 144 correct,\n",
      "    Accuacy is: 0.72%\n",
      "    Precision is: 0.75%\n",
      "    Recall is: 0.68%\n",
      "    F1 Score is: 0.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexguanga/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pos: 0\n",
      "false pos: 0\n",
      "true neg: 97\n",
      "false neg: 103\n",
      "\n",
      "\n",
      "    Out of 200 reviews, the model got 97 correct,\n",
      "    Accuacy is: 0.48%\n",
      "    Precision is: nan%\n",
      "    Recall is: 0.00%\n",
      "    F1 Score is: nan%\n",
      "true pos: 0\n",
      "false pos: 0\n",
      "true neg: 97\n",
      "false neg: 103\n",
      "\n",
      "\n",
      "    Out of 200 reviews, the model got 97 correct,\n",
      "    Accuacy is: 0.48%\n",
      "    Precision is: nan%\n",
      "    Recall is: 0.00%\n",
      "    F1 Score is: nan%\n",
      "true pos: 0\n",
      "false pos: 0\n",
      "true neg: 97\n",
      "false neg: 103\n",
      "\n",
      "\n",
      "    Out of 200 reviews, the model got 97 correct,\n",
      "    Accuacy is: 0.48%\n",
      "    Precision is: nan%\n",
      "    Recall is: 0.00%\n",
      "    F1 Score is: nan%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# We will be looking at other kernels\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for kernel in kernels:\n",
    "    classifier = SVC(kernel=kernel, random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Checking for the results\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    total = tn+fp+fn+tp\n",
    "    accuracy = (tn+tp)/total\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    \n",
    "    print(\"true pos: {0}\\n\"\n",
    "          \"false pos: {1}\\n\"\n",
    "          \"true neg: {2}\\n\"\n",
    "          \"false neg: {3}\\n\".format(tp, fp, tn, fn))\n",
    "    \n",
    "    print(\"\"\"\n",
    "    Out of {0} reviews, the model got {1} correct,\n",
    "    Accuacy is: {2:.2f}%\n",
    "    Precision is: {3:.2f}%\n",
    "    Recall is: {4:.2f}%\n",
    "    F1 Score is: {5:.2f}%\"\"\".format(total, tn+tp, accuracy, precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the classifier to the training set\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the values of the testing set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pos: 48\n",
      "false pos: 23\n",
      "true neg: 74\n",
      "false neg: 55\n",
      "\n",
      "\n",
      "Out of 200 reviews, the model got 122 correct,\n",
      "Accuacy is: 0.61%\n",
      "Precision is: 0.68%\n",
      "Recall is: 0.47%\n",
      "F1 Score is: 0.55%\n"
     ]
    }
   ],
   "source": [
    "# Checking for the results\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "total = tn+fp+fn+tp\n",
    "accuracy = (tn+tp)/total\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"true pos: {0}\\n\"\n",
    "    \"false pos: {1}\\n\"\n",
    "    \"true neg: {2}\\n\"\n",
    "    \"false neg: {3}\\n\".format(tp, fp, tn, fn))\n",
    "\n",
    "print(\"\"\"\n",
    "Out of {0} reviews, the model got {1} correct,\n",
    "Accuacy is: {2:.2f}%\n",
    "Precision is: {3:.2f}%\n",
    "Recall is: {4:.2f}%\n",
    "F1 Score is: {5:.2f}%\"\"\".format(total, tn+tp, accuracy, precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the classifier to the training set\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the values of the testing set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pos: 66\n",
      "false pos: 21\n",
      "true neg: 76\n",
      "false neg: 37\n",
      "\n",
      "\n",
      "Out of 200 reviews, the model got 142 correct,\n",
      "Accuacy is: 0.71%\n",
      "Precision is: 0.76%\n",
      "Recall is: 0.64%\n",
      "F1 Score is: 0.69%\n"
     ]
    }
   ],
   "source": [
    "# Checking for the results\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "total = tn+fp+fn+tp\n",
    "accuracy = (tn+tp)/total\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"true pos: {0}\\n\"\n",
    "    \"false pos: {1}\\n\"\n",
    "    \"true neg: {2}\\n\"\n",
    "    \"false neg: {3}\\n\".format(tp, fp, tn, fn))\n",
    "\n",
    "print(\"\"\"\n",
    "Out of {0} reviews, the model got {1} correct,\n",
    "Accuacy is: {2:.2f}%\n",
    "Precision is: {3:.2f}%\n",
    "Recall is: {4:.2f}%\n",
    "F1 Score is: {5:.2f}%\"\"\".format(total, tn+tp, accuracy, precision, recall, f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
