{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 28: Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 178: Thompson Sampling Intuition\n",
    "- Reiterating the problem of the casino problem (The Multi-Armed Bandit Problem)\n",
    "- <img src=\"../../images/AZ_Rienforcing_2.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "- The slide in the bottom will not explained a lot. It will be explained in the coding tutorial!\n",
    "- <img src=\"../../images/AZ-Thomas.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "- The actual return of each ad in this case! But when you are playing the game, you do not know the actual expected return!\n",
    "- <img src=\"../../images/AZ-Thomas-2.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "- For this case, we are not trying to predict where the expected value is. Instead, using the cases, we are a distribution to predict the expected mean value! The Thomas Sampling is problistic algorithm!\n",
    "- WE ARE NOT TRYING TO GUESS THE DISTRIBUTION OF THE MACHINES. WE ARE USING THE SAMPLES TO CREATE A DISTRIBUTION THAT IS NOT GUESSING BUT THE OBSERVED!\n",
    "- <img src=\"../../images/AZ-Thomas-3.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "- They are pulling values from the distribution (I'm guessing it does not have take out the mean but a value from the distribution)\n",
    "- Thus, we now created our own bandit configuration! \n",
    "- In the example below, we choose the green machine because it's further to the right (highest return)\n",
    "- <img src=\"../../images/AZ-Thomas-4.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "- You then compare it to the actual result which is the true expected value! You have a prior prob. the initial distribution, you then add the result to fix our own distribution. We narrow our distribution!\n",
    "- <img src=\"../../images/AZ-Thomas-5.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "- <img src=\"../../images/AZ-Thomas-6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 179: Algorithm Comparison: UCB vs Thompson Sampling\n",
    "- UCM\n",
    "    - UCM is deterministic. Meaning, that once you have the confidence bounds, there's no randomness is choosing the best ad or lever (whatever the case might be).\n",
    "    - After every round, you must update your model since we want to incorporate the average and upper bound using the new round!\n",
    "    - \n",
    "- Thomas Sampling\n",
    "    - Thomas Sampling is problistic because we generate random values from the distributions!\n",
    "    - Can accomadate delayed feedback! Meaning, you can update your model in batches which be less computational expensive!\n",
    "    - Has better empirical evidence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
