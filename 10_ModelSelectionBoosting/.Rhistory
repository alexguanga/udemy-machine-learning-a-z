?sample
set.seed(1)
training.set <- sample(1:nrow(x), nrow(x)/2)
testing.set <- (-training.set)
set.seed(1)
training.set <- sample(1:nrow(X), nrow(X)/2)
testing.set <- (-training.set)
testing.set
y.test <- y[testing.set]
y.test
install.packages("glmnet")
library(glmnet)
# Remember, we need to pass values for the lambda parameter
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod)) # There are 20 rows (one for each predictor) & 100 columns (one for each lambda)
ridge.mod
set.seed(1)
training.sample <- sample(1:nrow(X), nrow(X)/2)
testing.sample <- (-training.sample)
y.test <- y[testing.sample]
1e-12
ridge.mod <- glmnet(X[training.sample], y[training.sample],
alpha = 0, lambda = grid, thresh = 1e-12)
ridge.predict <- predict(ridge.mod, s = 4, newx = x[testing.sample, ])
mean((ridge.predict - y.test)^2)
ridge.mod <- glmnet(X[training.sample], y[training.sample],
alpha = 0, lambda = grid, thresh = 1e-12)
ridge.mod <- glmnet(X[training.sample,], y[training.sample],
alpha = 0, lambda = grid, thresh = 1e-12)
ridge.predict <- predict(ridge.mod, s = 4, newx = x[testing.sample, ])
ridge.predict <- predict(ridge.mod, s = 4, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2)
set.seed(1)
cv.out <- cv.glmnet(X[training.sample, ], y[training.sample], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.predict <- predict(ridge.mod, s = bestlam, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2)
?glmnet
out <- glmnet(X, y, alpha = 0)
predict(out, type = 'coefficients', s = bestlam)[1:20, ]
predict(out, type = 'coefficients', s = bestlam)
predict(out, type = 'coefficients', s = bestlam)[1:20, ]
lasso.mod <- glmnet(X[training.sample,], y[training.sample],
alpha = 1, lambda = grid)
plot(lasso.mod)
set.seed(1)
cv.out <- cv.glmnet(X[training.sample, ], y[training.sample], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min #211.7416
bestlam
ridge.predict <- predict(ridge.mod, s = bestlam, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2) #96015.51
set.seed(1)
cv.out <- cv.glmnet(X[training.sample, ], y[training.sample], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min #16.780166
ridge.predict <- predict(ridge.mod, s = bestlam, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2) #97244.28
set.seed(1)
cv.out <- cv.glmnet(X[training.sample, ], y[training.sample], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min #16.780166
ridge.predict <- predict(lasso.mod, s = bestlam, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2) #97244.28
# the lasso model with Î» chosen by cross-validation contains only seven variables.
out <- glmnet(X, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = bestlam)[1:20,]
out <- glmnet(X, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef !=0]
install.packages("pls")
# 6.7.1 Principal Components Regression
library(pls)
# 6.7.1 Principal Components Regression
library(pls)
pcr.fit <- pcr(Salary~., data = Hitters, scale = TRUE, validation = 'CV')
summary(pcr.fit)
setwd("~/All_Projects/ds-education/OnlineCourses/AZMachineLearning/10_ModelSelectionBoosting")
install.packages("xgboost")
library(XGBoost)
library(xgboost)
?xgboost
data
df <- read.csv("./archive/Churn_Modelling.csv")
# Focusing on the variable that can have an impact
# Eliminating Row Number, CustomerID, and Surname
df <- df[4:14]
df
df <- read.csv("./archive/Churn_Modelling.csv")
# Focusing on the variable that can have an impact
# Eliminating Row Number, CustomerID, and Surname
df <- df[4:14]
View(df)
testing.set[ ,!(colnames(testing.set) == "Exited")]
df <- read.csv("./archive/Churn_Modelling.csv")
# Focusing on the variable that can have an impact
# Eliminating Row Number, CustomerID, and Surname
df <- df[4:14]
# Encoding the categorical variables as factors
df$Geography <- as.numeric(factor(df$Geography,
levels = c('France', 'Germany', 'Spain'),
labels = c(1, 2, 3)))
df$Gender <- as.numeric(factor(df$Gender,
levels = c('Female', 'Male'),
labels = c(1, 2)))
library(caTools)
set.seed(123)
split <- sample.split(df$Exited, SplitRatio = 0.8)
training.set <- subset(df, split == TRUE)
testing.set <- subset(df, split == FALSE)
# Checking the datatype of the variables
sapply(testing.set, class)
testing.set[ ,!(colnames(testing.set) == "Exited")]
library(xgboost)
classifier <- xgboost(data = as.matrix(testing.set[ ,!(colnames(testing.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
testing.set[ ,!(colnames(testing.set) == "Exited")]
library(xgboost)
classifier <- xgboost(data = as.matrix(testing.set[ ,!(colnames(testing.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
library(xgboost)
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(testing.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
# Function for recall, precision, f1.score, accuracy
measurePrecisionRecall <- function(actual_labels, predict){
confusion.matrix <- table(actual_labels, predict)
tn <- table(actual_labels, predict)[1]
fn <- table(actual_labels, predict)[2]
fp <- table(actual_labels, predict)[3]
tp <- table(actual_labels, predict)[4]
total <- tn+fn+fp+tp
accuracy <- (tn+tp)/total
precision <- tp/(tp+fp)
recall <- tp/(tp+fn)
f1.score <- 2*precision*recall/(precision+recall)
cat("Accuracy", accuracy, '\n')
cat("Precision", precision, '\n')
cat("Recall", recall, '\n')
cat("F1 Score", f1.score, '\n', '----', '\n')
return(c(accuracy, precision, recall, f1.score))
}
library(xgboost)
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(testing.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
library(caret)
folds <- createFolds(training.set$Exited, k = 10)
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(testing.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Purchased)
accuracy <- performance.metrics[1]
return(accuracy)
})
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Purchased)
accuracy <- performance.metrics[1]
return(accuracy)
})
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
return(accuracy)
})
accuracy <- mean(as.numeric(cv))
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
return(accuracy)
})
as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")])
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
return(accuracy)
}
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
return(accuracy)
}
accuracy <- mean(as.numeric(cv)) #0.90625
accuracy <- mean(as.numeric(cv)) #0.90625
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
return(accuracy)
}
accuracy <- mean(as.numeric(cv)) #0.90625
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
return(accuracy)
}
accuracy <- mean(as.numeric(cv)) #0.90625
accuracy <- mean(as.numeric(cv)) #0.90625
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
return(accuracy)
}
cv
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
return(accuracy)
})
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
#return(accuracy)
})
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
# performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
# accuracy <- performance.metrics[1]
#return(accuracy)
})
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
# y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
# y.pred <- (y.pred >= 0.5)
# confusion.matrix <- table(testing.fold$Exited, y.pred)
#
# performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
# accuracy <- performance.metrics[1]
#return(accuracy)
})
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[ ,(colnames(testing.set) == "Exited")]))
# y.pred <- (y.pred >= 0.5)
# confusion.matrix <- table(testing.fold$Exited, y.pred)
#
# performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
# accuracy <- performance.metrics[1]
#return(accuracy)
})
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold$Exited))
# y.pred <- (y.pred >= 0.5)
# confusion.matrix <- table(testing.fold$Exited, y.pred)
#
# performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
# accuracy <- performance.metrics[1]
#return(accuracy)
})
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[-11]))
# y.pred <- (y.pred >= 0.5)
# confusion.matrix <- table(testing.fold$Exited, y.pred)
#
# performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
# accuracy <- performance.metrics[1]
#return(accuracy)
})
testing.fold[-11]
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[-11]))
print(testing.fold[-11])
# y.pred <- (y.pred >= 0.5)
# confusion.matrix <- table(testing.fold$Exited, y.pred)
#
# performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
# accuracy <- performance.metrics[1]
#return(accuracy)
})
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- xgboost(data = as.matrix(training.set[ ,!(colnames(training.set) == "Exited")]),
label = training.set$Exited,
nrounds = 10)
y.pred <- predict(classifier, newdata = as.matrix(testing.fold[-11])) # 11 is the dependent variable
y.pred <- (y.pred >= 0.5)
confusion.matrix <- table(testing.fold$Exited, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Exited)
accuracy <- performance.metrics[1]
return(accuracy)
})
accuracy <- mean(as.numeric(cv)) #0.90625
accuracy
