{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 31: ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 212: The Neuron\n",
    "- Axons, Dentrites, and Neurons\n",
    "- Axons are connected to other dentrities!\n",
    "- You generally want the independent variables to be the same.\n",
    "- Output value can be continous (price), binary (yes/no), categorical (discrete).\n",
    "- Think of it like a Multi-Linear Regression where there are many indepedent variables that are for one given possiblity.\n",
    "- The synapses here are the weighted (or the line the connect different layers)\n",
    "- The secret is inside the neuron or the brain.\n",
    "- 1st steps: Adds the summation of the weights * input variable\n",
    "- 2nd steps: Performs some kind of calcualtion, a sigmoid function can be a good one because it provides a number between 0 and 1.\n",
    "- <img src=\"../../images/AZ_DeepLearning1.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "- **STEPS**\n",
    "    - <img src=\"../../images/AZ_DeepLearning2.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "    - <img src=\"../../images/AZ_DeepLearning3.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 213: The Activation Function\n",
    "- We will look at four type of activation functions.\n",
    "    - **Threshold Function:** <img src=\"../../images/DeepLearning_3_1.png\" alt=\"Drawing\" style=\"width: 400px;\" >\n",
    "    - If the value is less than 0, then the function passes 0.\n",
    "    - If the value is greater than 0, then the function passes 1.\n",
    "    - **Sigmoid Function:** <img src=\"../../images/DeepLearning_3_2.png\" alt=\"Drawing\" style=\"width: 400px;\" >\n",
    "    - Smooth graph, this is good when predicting probability\n",
    "    - **Rectifier:** <img src=\"../../images/DeepLearning_3_3.png\" alt=\"Drawing\" style=\"width: 400px;\" >\n",
    "    - Anything below 0, like a negative number, will be 0. From there, it will increase in a linear\n",
    "    - **Hyperbolic Tangent:** <img src=\"../../images/DeepLearning_3_4.png\" alt=\"Drawing\" style=\"width: 400px;\" >\n",
    "    - This function is similar to the sigmoid but the values below 0 can go to negative 1.\n",
    "\n",
    "- The hidden layer can have a rectifier function. The signal would be created and passed into the sigmoid function! NEAT!\n",
    "- If the dependent variable is a binary, we can use the Threshold Function or the Sigmoid Function\n",
    "\n",
    "- A common blue print of the methology behind these neural networks:\n",
    "    - <img src=\"../../images/AZ_DeepLearning4.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 214: How do Neural Networks Work?\n",
    "- We would like build neural networks to find the property valuation. The inmut are inmput variables which will output the valuation of the property!\n",
    "- There's a power behding the hidden layers. It is supposed to give us an extra power!\n",
    "- <img src=\"../../images/AZ_DeepLearning5.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- For the nueron above, the only two variables that are linked are area and distance to miles. But the other two does not provide any values. The nueron looks for the factor that only focuses on the distance and area. The neuron picks out.\n",
    "- <img src=\"../../images/AZ_DeepLearning6.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- For the example above, we are thinking about families that are looking for large property! We can think tht a neuron is build to look at new houses that has a lot of rooms with a lot of area. Hence, the reason that this neuron only focuses on the variables described in the image above!\n",
    "- <img src=\"../../images/AZ_DeepLearning7.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- The image above can only look at one variable, age. Age can look at the houses. For example, after a certain age, houses might be worth more money. Under that threshold, houses might mean that they are less value! For example, it can follow a rectifier function, under 100 is not activated but after 100, it will get activated!\n",
    "- Using these neural networks, we can find hidden variables (or variable), that can depict some kind of findings that we do not not have otherwise. This increases the flexibility that we have! Then \n",
    "- <img src=\"../../images/AZ_DeepLearning8.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 215: How do Neural Networks learn?\n",
    "- There are two fundamental apporaches for this:\n",
    "    - You can either provide the exact instructions!\n",
    "    - You can either provide the inputs and outputs and then it figures it out by itself. It pretty much learns on it own!\n",
    "- For example, if we have dogs and cats, we can use labels of pictures! & then the model will look at the images, and learn the approach!\n",
    "- <img src=\"../../images/AZ_DeepLearning10.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "- The image above is an example of a perceptron!\n",
    "- The cost function is the same as it always is! \n",
    "    - C = 1/2(Predicted Y - Actual) ^ 2\n",
    "- We then feed the cost function back the neural network (the weights get updated by using the gradient descent). We find the optimal ways to tweak the weights!\n",
    "- <img src=\"../../images/AZ_DeepLearning12.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "- The ex below shows how we use the variables to makea prediction and then we focus on the error. Based on the error, we calculate the gradient descent!\n",
    "- <img src=\"../../images/AZ_DeepLearning13.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "- One epoch is when you train throughout all the dataset!\n",
    "- Cost function is the sum all of the differences!\n",
    "- <img src=\"../../images/AZ_DeepLearning14.png\" alt=\"Drawing\" style=\"width: 800px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 216: Gradient Descent\n",
    "- <img src=\"../../images/AZ_DeepLearning15.png\" alt=\"Drawing\" style=\"width: 600px;\">\n",
    "- If we have 25 weighted with 1000 combination, using all of them, requries a lot of computation time (more than the universe has existed)\n",
    "- <img src=\"../../images/AZ_DeepLearning16.png\" alt=\"Drawing\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 217: Stochastic Gradient Descent\n",
    "- <img src=\"../../images/AZ_DeepLearning17.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "- The cost function does not have to be a perfect convec figure. For example, we might use another cost function and even the one we use, the function in a higher-dimension can be described as the image above!\n",
    "- <img src=\"../../images/AZ_DeepLearning18.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "- Like the image above, we might find the incorrect minima!\n",
    "\n",
    "- **Batch Gradient Descent**\n",
    "    - You have one neuron network and trained it for the entire batch. Hence, we find the cost function (the summation of all the observation) and then adjust the weights based on that!\n",
    "    - Or called the batch, because we perform the cost function to every observation in the batch and ADJUST it it on the entire batch!\n",
    "- **Stochastic Gradient Descent**\n",
    "    - Here, we train it on one observation AND adjust the weights based on the cost function for that observation (which is done for every example)\n",
    "\n",
    "- <img src=\"../../images/AZ_DeepLearning19.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "\n",
    "\n",
    "- ** The Stochastic Gradient Descent is quicker than the Batch Gradient Descent**\n",
    "    - Because the stochastic adjust the weights after every row, it does not have to store the info into memory like the batch gradient descent, which is why the batch gradient descent takes a longer time.\n",
    "- The Stochastic might be more random than the batch!\n",
    "\n",
    "- **Mini-Batch Gradient Descent**\n",
    "    - In btw both gradient descent, where you focus on mini-batches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 218: Backpropogation\n",
    "- **Forward Propogation**\n",
    "    - Info. is inserted into the neural networks! We then calculate the error (using the cost function)\n",
    "    - <img src=\"../../images/AZ_DeepLearning20.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "\n",
    "- **Back Propogation**\n",
    "    - The erros are put back and we need to use the gradient descent! \n",
    "    - All the weights are adjusted at the same time\n",
    "    - Hence, you can figure out which weights has more influenced\n",
    "    - <img src=\"../../images/AZ_DeepLearning21.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "\n",
    "    \n",
    "- <img src=\"../../images/AZ_DeepLearning22.png\" alt=\"Drawing\" style=\"width: 700px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
